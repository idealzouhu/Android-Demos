一、ASR 原理简介

### 1.1 ASR 基本原理

自动语音识别（Automatic Speech Recognition, ASR）是将人类语音转换为文本的技术。其基本流程包括：

1. **音频预处理**：将原始音频信号转换为特征向量（如 Mel 频谱特征）
2. **声学模型**：使用深度学习模型（如 Transformer、LSTM、Zipformer 等）将特征向量映射为音素或字符序列
3. **语言模型**：结合语言知识，提高识别准确率
4. **解码**：将模型输出转换为最终的文本结果

### 1.2 OnlineRecognizer 与 OfflineRecognizer 的区别

本项目采用**两阶段识别架构**，结合了在线识别器和离线识别器的优势：

#### OnlineRecognizer（在线识别器）

- **特点**：流式识别，实时处理音频流
- **适用场景**：实时语音转写、语音助手、实时字幕
- **优势**：
  - 低延迟，可以边录音边识别
  - 支持端点检测（endpoint detection），自动判断语音是否结束
  - 支持增量解码，实时显示识别结果
- **模型类型**：使用流式模型（streaming models），模型名称通常包含 "streaming" 关键字
- **API 特性**：
  - `isEndpoint()`：检测语音是否结束
  - `isReady()`：检查是否准备好进行解码
  - `reset()`：重置识别状态，开始新的识别会话

#### OfflineRecognizer（离线识别器）

- **特点**：非流式识别，处理完整音频片段
- **适用场景**：对完整音频进行精确识别、后处理优化
- **优势**：
  - 识别准确率通常更高（可以查看完整上下文）
  - 支持更多模型类型（Whisper、Paraformer、SenseVoice 等）
  - 返回更丰富的信息（语言、情感、事件等）
- **模型类型**：使用非流式模型（offline models）
- **API 特性**：
  - `getResult()`：返回包含文本、语言、情感等信息的完整结果
  - 不支持端点检测，需要手动控制音频输入

#### 两阶段识别流程

在本项目中，两阶段识别的工作流程如下：

1. **第一阶段（OnlineRecognizer）**：
   - 实时接收音频流
   - 进行流式识别，实时显示初步结果
   - 检测语音端点（endpoint detection）
   - 当检测到端点时，触发第二阶段

2. **第二阶段（OfflineRecognizer）**：
   - 接收第一阶段的完整音频片段
   - 使用离线模型进行更精确的识别
   - 返回最终优化后的识别结果

这种设计兼顾了**实时性**和**准确性**：第一阶段提供快速反馈，第二阶段提供精确结果。



二、so 库和模型文件
### 2.1 SO 库介绍

项目使用以下 JNI 库（位于 `app/src/main/jniLibs/` 目录）：

- **libsherpa-onnx-jni.so**：SherpaOnnx 的 JNI 接口库，提供 Kotlin/Java 与底层 C++ 代码的桥接
- **libonnxruntime.so**：ONNX Runtime 库，用于执行 ONNX 模型推理

这些库支持以下架构：

- `arm64-v8a`：64位 ARM 架构（现代 Android 设备）
- `armeabi-v7a`：32位 ARM 架构（较旧的 Android 设备）
- `x86`、`x86_64`：x86 架构（模拟器）

下载链接为 https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.23/sherpa-onnx-v1.12.23-android.tar.bz2


### 2.2 模型文件

#### 2.2.1 模型文件下载

模型文件需要放置在 `app/src/main/assets/` 目录下。项目支持多种不同语言、不同模型架构、流式和非流式的预训练模型。

**流式模型（用于 OnlineRecognizer）**：

- `sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23`：中文 Zipformer 模型（14M 参数）
  - 下载地址：https://huggingface.co/csukuangfj/sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23
- `sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20`：中英双语 Zipformer 模型
  - 下载地址：https://huggingface.co/csukuangfj/sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20
- `sherpa-onnx-streaming-paraformer-bilingual-zh-en`：中英双语 Paraformer 模型
  - 下载地址：https://huggingface.co/csukuangfj/sherpa-onnx-streaming-paraformer-bilingual-zh-en
- `sherpa-onnx-lstm-zh-2023-02-20`：中文 LSTM 模型
  - 下载地址：https://huggingface.co/csukuangfj/sherpa-onnx-lstm-zh-2023-02-20
- `sherpa-onnx-lstm-en-2023-02-17`：英文 LSTM 模型
  - 下载地址：https://huggingface.co/csukuangfj/sherpa-onnx-lstm-en-2023-02-17

**离线模型（用于 OfflineRecognizer）**：

- `sherpa-onnx-paraformer-zh-2023-09-14`：中文 Paraformer 模型
  - 下载地址：https://huggingface.co/csukuangfj/sherpa-onnx-paraformer-zh-2023-09-14
- `sherpa-onnx-whisper-tiny.en`：英文 Whisper 模型（小版本）
  - 下载地址：https://huggingface.co/csukuangfj/sherpa-onnx-whisper-tiny.en
- `sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17`：多语言 SenseVoice 模型
  - 下载地址：https://huggingface.co/csukuangfj/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-int8-2024-07-17

> **提示**：
> - 更多预训练模型请参考 [官方预训练模型列表](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html)
> - 模型文件通常较大（几十MB到几百MB），请确保有足够的存储空间
> - 下载后解压，将整个模型目录（如 `sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23`）放入 `app/src/main/assets/` 目录

解析下载的离线模型压缩包 [sherpa-onnx-zipformer-zh-en-2023-11-22.tar.bz2](https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-zipformer-zh-en-2023-11-22.tar.bz2)，我们可以看到如下内容：

```Bash
$ ls -lh
total 324M
-rw-r--r-- 1 zouhu 197121  265K Nov 22  2023 bbpe.model
-rw-r--r-- 1 zouhu 197121  5.0M Nov 22  2023 decoder-epoch-34-avg-19.onnx
-rw-r--r-- 1 zouhu 197121   66M Nov 22  2023 encoder-epoch-34-avg-19.int8.onnx
-rw-r--r-- 1 zouhu 197121  248M Nov 22  2023 encoder-epoch-34-avg-19.onnx
-rw-r--r-- 1 zouhu 197121 1010K Nov 22  2023 joiner-epoch-34-avg-19.int8.onnx
-rw-r--r-- 1 zouhu 197121  4.0M Nov 22  2023 joiner-epoch-34-avg-19.onnx
drwxr-xr-x 1 zouhu 197121     0 Dec 24  2024 test_wavs/
-rw-r--r-- 1 zouhu 197121   26K Dec 24  2024 tokens.txt

```


### 2.2.2 模型文件结构

每个模型目录通常包含以下文件：

**Transducer 模型**：

- `encoder-*.onnx`：编码器模型
- `decoder-*.onnx`：解码器模型
- `joiner-*.onnx`：连接器模型
- `tokens.txt`：词汇表文件

**Paraformer 模型**：

- `encoder-*.onnx`：编码器模型
- `decoder-*.onnx`：解码器模型
- `tokens.txt`：词汇表文件

**CTC 模型**：

- `model.onnx` 或 `model.int8.onnx`：单文件模型
- `tokens.txt`：词汇表文件

**可选文件**：

- `with-state-*.onnx`：语言模型（用于提升识别准确率）
- `lexicon.txt`：词典文件（用于同音字替换）
- `replace.fst`：FST 规则文件（用于同音字替换）


三、so 库接口和主要组件

### 3.1 核心组件

#### 3.1.1 OnlineRecognizer（在线识别器）

在线识别器用于流式语音识别，主要类和方法如下：

**类定义**：
```kotlin
class OnlineRecognizer(
    assetManager: AssetManager? = null,
    val config: OnlineRecognizerConfig,
)
```

**主要方法**：

- `createStream(hotwords: String = "")`：创建识别流
  - 返回：`OnlineStream` 对象
  - 参数：`hotwords` 为热词（可选）

- `decode(stream: OnlineStream)`：对音频流进行解码
  - 参数：`stream` 为识别流对象

- `isReady(stream: OnlineStream)`：检查是否准备好进行解码
  - 返回：`Boolean`，表示是否可以解码

- `isEndpoint(stream: OnlineStream)`：检测是否到达语音端点
  - 返回：`Boolean`，表示是否检测到语音结束

- `getResult(stream: OnlineStream)`：获取识别结果
  - 返回：`OnlineRecognizerResult`，包含文本、tokens、时间戳等

- `reset(stream: OnlineStream)`：重置识别流，开始新的识别会话

- `release()`：释放资源

#### 3.1.2 OfflineRecognizer（离线识别器）

离线识别器用于非流式语音识别，主要类和方法如下：

**类定义**：
```kotlin
class OfflineRecognizer(
    assetManager: AssetManager? = null,
    val config: OfflineRecognizerConfig,
)
```

**主要方法**：

- `createStream()`：创建识别流
  - 返回：`OfflineStream` 对象

- `decode(stream: OfflineStream)`：对音频进行解码
  - 参数：`stream` 为识别流对象

- `getResult(stream: OfflineStream)`：获取识别结果
  - 返回：`OfflineRecognizerResult`，包含文本、tokens、时间戳、语言、情感、事件等丰富信息

- `release()`：释放资源

#### 3.1.3 OnlineStream / OfflineStream（识别流）

识别流用于管理音频输入和识别状态：

**OnlineStream 主要方法**：

- `acceptWaveform(samples: FloatArray, sampleRate: Int)`：接收音频数据
  - 参数：
    - `samples`：音频样本数组（归一化到 [-1.0, 1.0]）
    - `sampleRate`：采样率（通常为 16000 Hz）

- `inputFinished()`：标记输入完成

- `release()`：释放流资源

**OfflineStream 主要方法**：

- `acceptWaveform(samples: FloatArray, sampleRate: Int)`：接收音频数据
- `release()`：释放流资源

### 3.2 配置类

#### 3.2.1 OnlineRecognizerConfig

在线识别器配置，包含以下主要字段：

- `featConfig: FeatureConfig`：特征提取配置（采样率、特征维度等）
- `modelConfig: OnlineModelConfig`：模型配置（模型路径、类型等）
- `endpointConfig: EndpointConfig`：端点检测配置
- `enableEndpoint: Boolean`：是否启用端点检测
- `decodingMethod: String`：解码方法（如 "greedy_search"）
- `hotwordsFile: String`：热词文件路径
- `ruleFsts: String`：FST 规则文件路径

#### 3.2.2 OfflineRecognizerConfig

离线识别器配置，包含以下主要字段：

- `featConfig: FeatureConfig`：特征提取配置
- `modelConfig: OfflineModelConfig`：模型配置
- `decodingMethod: String`：解码方法
- `hotwordsFile: String`：热词文件路径
- `ruleFsts: String`：FST 规则文件路径

### 3.3 结果类

#### 3.3.1 OnlineRecognizerResult

在线识别结果，包含：

- `text: String`：识别文本
- `tokens: Array<String>`：识别出的 token 序列
- `timestamps: FloatArray`：时间戳数组
- `ysProbs: FloatArray`：概率数组

#### 3.3.2 OfflineRecognizerResult

离线识别结果，包含：

- `text: String`：识别文本
- `tokens: Array<String>`：识别出的 token 序列
- `timestamps: FloatArray`：时间戳数组
- `lang: String`：检测到的语言
- `emotion: String`：检测到的情感
- `event: String`：检测到的事件
- `durations: FloatArray`：持续时间数组（仅 TDT 模型支持）



四、简单实现案例

### 4.1 实现流程

两阶段语音识别的实现流程如下：

1. **初始化阶段**：
   - 初始化 `OnlineRecognizer`（第一遍识别器）
   - 初始化 `OfflineRecognizer`（第二遍识别器）
   - 初始化音频录制器（`AudioRecord`）

2. **录音和识别阶段**：
   - 创建 `OnlineStream` 流
   - 循环读取音频数据
   - 将音频数据送入在线识别器进行实时识别
   - 检测语音端点（endpoint）
   - 当检测到端点时，使用离线识别器进行精确识别

3. **结果处理**：
   - 显示实时识别结果（第一遍）
   - 显示最终识别结果（第二遍）

### 4.2 完整代码示例

以下是一个简化的实现示例，展示了如何使用两阶段识别：

```kotlin
class MainActivity : AppCompatActivity() {
    private lateinit var onlineRecognizer: OnlineRecognizer
    private lateinit var offlineRecognizer: OfflineRecognizer
    private var audioRecord: AudioRecord? = null
    private val sampleRateInHz = 16000
    private var isRecording = false
    private var samplesBuffer = arrayListOf<FloatArray>()

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)

        // 初始化识别器
        initOnlineRecognizer()
        initOfflineRecognizer()

        // 设置录音按钮点击事件
        findViewById<Button>(R.id.record_button).setOnClickListener {
            if (!isRecording) {
                startRecording()
            } else {
                stopRecording()
            }
        }
    }

    // 初始化在线识别器
    private fun initOnlineRecognizer() {
        val config = OnlineRecognizerConfig(
            featConfig = getFeatureConfig(sampleRate = sampleRateInHz, featureDim = 80),
            modelConfig = getModelConfig(type = 9)!!, // 使用模型类型 9
            endpointConfig = getEndpointConfig(),
            enableEndpoint = true,
        )

        onlineRecognizer = OnlineRecognizer(
            assetManager = application.assets,
            config = config,
        )
    }

    // 初始化离线识别器
    private fun initOfflineRecognizer() {
        val config = OfflineRecognizerConfig(
            featConfig = getFeatureConfig(sampleRate = sampleRateInHz, featureDim = 80),
            modelConfig = getOfflineModelConfig(type = 0)!!, // 使用模型类型 0
        )

        offlineRecognizer = OfflineRecognizer(
            assetManager = application.assets,
            config = config,
        )
    }

    // 开始录音和识别
    private fun startRecording() {
        // 初始化音频录制器
        val bufferSize = AudioRecord.getMinBufferSize(
            sampleRateInHz,
            AudioFormat.CHANNEL_IN_MONO,
            AudioFormat.ENCODING_PCM_16BIT
        )

        audioRecord = AudioRecord(
            MediaRecorder.AudioSource.MIC,
            sampleRateInHz,
            AudioFormat.CHANNEL_IN_MONO,
            AudioFormat.ENCODING_PCM_16BIT,
            bufferSize * 2
        )

        audioRecord?.startRecording()
        isRecording = true
        samplesBuffer.clear()

        // 在后台线程处理音频
        thread {
            processSamples()
        }
    }

    // 停止录音
    private fun stopRecording() {
        isRecording = false
        audioRecord?.stop()
        audioRecord?.release()
        audioRecord = null
    }

    // 处理音频样本
    private fun processSamples() {
        val stream = onlineRecognizer.createStream()
        val bufferSize = (0.1 * sampleRateInHz).toInt() // 100ms 的缓冲区
        val buffer = ShortArray(bufferSize)

        while (isRecording) {
            val ret = audioRecord?.read(buffer, 0, buffer.size) ?: 0
            if (ret > 0) {
                // 转换为浮点数组（归一化到 [-1.0, 1.0]）
                val samples = FloatArray(ret) { buffer[it] / 32768.0f }
                samplesBuffer.add(samples)

                // 送入在线识别器
                stream.acceptWaveform(samples, sampleRate = sampleRateInHz)
                
                // 解码
                while (onlineRecognizer.isReady(stream)) {
                    onlineRecognizer.decode(stream)
                }

                // 获取实时识别结果
                val result = onlineRecognizer.getResult(stream)
                runOnUiThread {
                    // 显示实时结果
                    findViewById<TextView>(R.id.my_text).text = result.text
                }

                // 检测端点
                if (onlineRecognizer.isEndpoint(stream)) {
                    onlineRecognizer.reset(stream)

                    // 使用离线识别器进行精确识别
                    val finalText = runSecondPass()
                    runOnUiThread {
                        findViewById<TextView>(R.id.my_text).text = finalText
                    }
                }
            }
        }

        stream.release()
    }

    // 第二遍识别（离线识别）
    private fun runSecondPass(): String {
        // 合并所有音频样本
        var totalSamples = 0
        for (a in samplesBuffer) {
            totalSamples += a.size
        }

        val samples = FloatArray(totalSamples)
        var i = 0
        for (a in samplesBuffer) {
            for (s in a) {
                samples[i] = s
                i++
            }
        }

        // 创建离线识别流
        val stream = offlineRecognizer.createStream()
        stream.acceptWaveform(samples, sampleRateInHz)
        offlineRecognizer.decode(stream)
        val result = offlineRecognizer.getResult(stream)

        stream.release()
        samplesBuffer.clear()

        return result.text
    }
}
```

### 4.3 关键要点

1. **音频格式**：
   - 采样率：16000 Hz
   - 声道：单声道（MONO）
   - 格式：16-bit PCM
   - 归一化：音频样本需要归一化到 [-1.0, 1.0] 范围

2. **端点检测**：
   - 使用 `isEndpoint()` 检测语音是否结束
   - 检测到端点后，调用 `reset()` 重置流，准备下一段识别

3. **资源管理**：
   - 使用完 `Stream` 后必须调用 `release()` 释放资源
   - 识别器使用完后也应调用 `release()` 释放资源

4. **线程安全**：
   - 音频处理应在后台线程进行
   - UI 更新使用 `runOnUiThread()` 切换到主线程

5. **模型选择**：
   - 第一遍（OnlineRecognizer）：选择流式模型，注重实时性
   - 第二遍（OfflineRecognizer）：选择离线模型，注重准确性



参考资料

- [Sherpa-Onnx 官方文档](https://k2-fsa.github.io/sherpa/onnx/index.html)
- [预训练模型列表](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html)
- [GitHub 仓库](https://github.com/k2-fsa/sherpa-onnx)
- [HuggingFace 模型仓库](https://huggingface.co/csukuangfj)
