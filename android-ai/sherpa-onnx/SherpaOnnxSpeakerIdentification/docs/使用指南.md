# SherpaOnnx 说话人识别使用指南

本文档介绍如何使用 SherpaOnnx 库实现说话人注册和识别功能。



## 一、说话人识别原理简介

说话人识别（Speaker Recognition）是一种通过分析语音特征来识别说话人身份的技术。

### 1.1 核心概念

1. **特征提取**：从音频信号中提取能够代表说话人独特特征的向量（称为"声纹特征"或"说话人嵌入"）
   - 每个人的声带、口腔结构、发音习惯不同，这些差异会体现在语音的频谱特征上
   - 深度学习模型可以将这些特征编码成固定维度的特征向量（通常 192-512 维）

2. **特征注册**：将说话人的特征向量存储到数据库中
   - 通常为每个说话人录制多段音频（3-5 段），提取多个特征向量
   - 这些特征向量代表了该说话人的"声纹模板"

3. **特征匹配**：将待识别音频的特征向量与已注册的特征向量进行相似度比较
   - 计算特征向量之间的余弦相似度或欧氏距离
   - 如果相似度超过预设阈值，则认为是同一说话人

### 1.2 工作流程

```
音频输入 → 特征提取 → 特征向量 → 相似度计算 → 识别结果
   ↓           ↓           ↓            ↓           ↓
录制音频   深度学习模型   固定维度向量   与模板比较   匹配/不匹配
```

### 1.3 技术特点

- **非侵入性**：只需录制语音，无需接触式采集
- **实时性**：可以实时处理音频流
- **准确性**：在安静环境下准确率较高
- **局限性**：受环境噪声、说话人状态（如感冒）等因素影响



## 二、依赖文件

### 2.1 SO 文件

SherpaOnnx 需要以下 JNI 库文件（`.so` 文件）：

- `libsherpa-onnx-jni.so` - SherpaOnnx JNI 接口库
- `libonnxruntime.so` - ONNX Runtime 库

这些文件需要放置在 Android 项目的 `src/main/jniLibs/` 目录下，按照不同的 CPU 架构组织：

```
jniLibs/
├── arm64-v8a/
│   ├── libsherpa-onnx-jni.so
│   └── libonnxruntime.so
├── armeabi-v7a/
│   ├── libsherpa-onnx-jni.so
│   └── libonnxruntime.so
├── x86/
│   ├── libsherpa-onnx-jni.so
│   └── libonnxruntime.so
└── x86_64/
    ├── libsherpa-onnx-jni.so
    └── libonnxruntime.so
```



### 2.2 ONNX 模型文件

需要下载说话人识别模型文件（`.onnx` 格式），推荐使用：

- **模型名称**: `3dspeaker_speech_eres2net_base_sv_zh-cn_3dspeaker_16k.onnx`
- **下载地址**: https://github.com/k2-fsa/sherpa-onnx/releases/tag/speaker-recongition-models
- **放置位置**: 
  - 如果从 Assets 加载：放置在 `src/main/assets/` 目录下（不要放在子目录中）
  - 如果从文件系统加载：放置在设备可访问的文件路径

这个模型命名里面不同字段的含义如下：







## 三、SherpaOnnx 库接口

**注意**：以下类必须位于 `com.k2fsa.sherpa.onnx` 包下才能正确调用 JNI 函数。

### 3.1 SpeakerEmbeddingExtractorConfig

配置类，用于配置说话人特征提取器的参数。

```kotlin
data class SpeakerEmbeddingExtractorConfig(
    val model: String = "",           // 模型文件路径或名称
    var numThreads: Int = 1,          // 线程数
    var debug: Boolean = false,       // 是否开启调试模式
    var provider: String = "cpu",     // 执行提供者：cpu, cuda, coreml 等
)
```

### 3.2 SpeakerEmbeddingExtractor

说话人特征提取器，用于从音频中提取说话人特征向量。

**主要方法**：

- `createStream(): OnlineStream` - 创建音频流对象
- `isReady(stream: OnlineStream): Boolean` - 检查流是否准备好提取特征
- `compute(stream: OnlineStream): FloatArray` - 计算并返回特征向量
- `dim(): Int` - 返回特征向量的维度
- `release()` - 释放资源

**构造函数**：

```kotlin
SpeakerEmbeddingExtractor(
    assetManager: AssetManager? = null,  // Android AssetManager（从 Assets 加载时使用）
    config: SpeakerEmbeddingExtractorConfig  // 配置对象
)
```

### 3.3 OnlineStream

音频流对象，用于处理音频数据。

**主要方法**：

- `acceptWaveform(samples: FloatArray, sampleRate: Int)` - 接收音频波形数据
- `inputFinished()` - 标记输入完成
- `release()` - 释放资源
- `use(block: (OnlineStream) -> Unit)` - 使用资源块，自动释放

### 3.4 SpeakerEmbeddingManager

说话人特征管理器，用于存储和管理已注册说话人的特征向量。

**主要方法**：

- `add(name: String, embedding: FloatArray): Boolean` - 添加单个特征向量
- `add(name: String, embedding: Array<FloatArray>): Boolean` - 添加多个特征向量（用于同一说话人的多个样本）
- `remove(name: String): Boolean` - 移除指定说话人
- `search(embedding: FloatArray, threshold: Float): String` - 搜索最相似的说话人，返回说话人名称（如果相似度低于阈值则返回空字符串）
- `verify(name: String, embedding: FloatArray, threshold: Float): Boolean` - 验证给定特征向量是否属于指定说话人
- `contains(name: String): Boolean` - 检查是否包含指定说话人
- `numSpeakers(): Int` - 返回已注册说话人数量
- `allSpeakerNames(): Array<String>` - 返回所有说话人名称数组
- `release()` - 释放资源

**构造函数**：

```kotlin
SpeakerEmbeddingManager(dim: Int)  // dim: 特征向量维度，应与 SpeakerEmbeddingExtractor.dim() 一致
```

### 3.5 SpeakerRecognition

单例对象，提供统一的初始化接口。

**主要方法**：

- `initExtractor(assetManager: AssetManager? = null)` - 初始化提取器 `SpeakerEmbeddingExtractor` 和管理器 `SpeakerEmbeddingManager`
- `extractor: SpeakerEmbeddingExtractor` - 获取提取器实例
- `manager: SpeakerEmbeddingManager` - 获取管理器实例



## 四、说话人注册

### 4.1 实现步骤

说话人注册的流程如下：

1. **初始化提取器和管理器**
2. **录制音频数据**
3. **创建音频流并输入音频数据**
4. **提取特征向量**
5. **将特征向量添加到管理器**

### 4.2 主要组件

- `SpeakerEmbeddingExtractor` - 用于提取特征向量
- `OnlineStream` - 用于处理音频流
- `SpeakerEmbeddingManager` - 用于存储说话人特征

### 4.3 示例代码

```kotlin
import android.content.res.AssetManager
import com.k2fsa.sherpa.onnx.*

// 1. 初始化（通常在应用启动时执行一次）
fun initializeSpeakerRecognition(assetManager: AssetManager?) {
    SpeakerRecognition.initExtractor(assetManager)
}

// 2. 说话人注册函数
fun registerSpeaker(
    speakerName: String,
    audioSamples: List<FloatArray>,  // 音频样本列表，每个 FloatArray 是一段音频数据
    sampleRate: Int = 16000
): Boolean {
    val extractor = SpeakerRecognition.extractor
    val manager = SpeakerRecognition.manager
    
    // 检查说话人是否已存在
    if (manager.contains(speakerName)) {
        println("说话人 $speakerName 已存在")
        return false
    }
    
    // 存储所有提取的特征向量
    val embeddingList = mutableListOf<FloatArray>()
    
    // 对每段音频提取特征向量
    for (samples in audioSamples) {
        // 创建音频流
        val stream = extractor.createStream()
        
        try {
            // 输入音频数据
            stream.acceptWaveform(samples, sampleRate)
            stream.inputFinished()
            
            // 检查是否准备好提取特征
            if (extractor.isReady(stream)) {
                // 提取特征向量
                val embedding = extractor.compute(stream)
                embeddingList.add(embedding)
            }
        } finally {
            // 释放流资源
            stream.release()
        }
    }
    
    // 将特征向量添加到管理器
    if (embeddingList.isNotEmpty()) {
        val success = manager.add(speakerName, embeddingList.toTypedArray())
        if (success) {
            println("成功注册说话人: $speakerName，共 ${embeddingList.size} 个特征向量")
        }
        return success
    }
    
    return false
}

// 使用示例
fun exampleRegister() {
    // 假设已录制多段音频数据
    val audioSamples = listOf(
        FloatArray(1600),  // 0.1秒的音频（16kHz采样率）
        FloatArray(1600),
        FloatArray(1600)
    )
    
    // 注册说话人
    registerSpeaker("张三", audioSamples, 16000)
}
```

## 五、说话人识别

### 5.1 实现步骤

说话人识别的流程如下：

1. **确保提取器和管理器已初始化**
2. **录制或获取待识别的音频数据**
3. **创建音频流并输入音频数据**
4. **提取特征向量**
5. **在管理器中搜索匹配的说话人**

### 5.2 主要组件

- `SpeakerEmbeddingExtractor` - 用于提取特征向量
- `OnlineStream` - 用于处理音频流
- `SpeakerEmbeddingManager` - 用于搜索匹配的说话人

### 5.3 示例代码

```kotlin
import com.k2fsa.sherpa.onnx.*

// 说话人识别函数
fun identifySpeaker(
    audioSamples: List<FloatArray>,  // 音频样本列表
    sampleRate: Int = 16000,
    threshold: Float = 0.5f  // 相似度阈值，范围通常为 0.0-1.0
): String? {
    val extractor = SpeakerRecognition.extractor
    val manager = SpeakerRecognition.manager
    
    // 检查是否有已注册的说话人
    if (manager.numSpeakers() == 0) {
        println("没有已注册的说话人")
        return null
    }
    
    // 合并所有音频样本（或处理单段音频）
    val stream = extractor.createStream()
    
    try {
        // 输入所有音频数据
        for (samples in audioSamples) {
            stream.acceptWaveform(samples, sampleRate)
        }
        stream.inputFinished()
        
        // 检查是否准备好提取特征
        if (extractor.isReady(stream)) {
            // 提取特征向量
            val embedding = extractor.compute(stream)
            
            // 搜索匹配的说话人
            val speakerName = manager.search(embedding, threshold)
            
            if (speakerName.isNotEmpty()) {
                println("识别结果: $speakerName")
                return speakerName
            } else {
                println("未找到匹配的说话人（相似度低于阈值 $threshold）")
                return null
            }
        } else {
            println("音频流未准备好")
            return null
        }
    } finally {
        // 释放流资源
        stream.release()
    }
}

// 验证说话人身份（1:1 验证）
fun verifySpeaker(
    speakerName: String,
    audioSamples: List<FloatArray>,
    sampleRate: Int = 16000,
    threshold: Float = 0.5f
): Boolean {
    val extractor = SpeakerRecognition.extractor
    val manager = SpeakerRecognition.manager
    
    // 检查说话人是否存在
    if (!manager.contains(speakerName)) {
        println("说话人 $speakerName 未注册")
        return false
    }
    
    val stream = extractor.createStream()
    
    try {
        // 输入音频数据
        for (samples in audioSamples) {
            stream.acceptWaveform(samples, sampleRate)
        }
        stream.inputFinished()
        
        if (extractor.isReady(stream)) {
            val embedding = extractor.compute(stream)
            
            // 验证身份
            val verified = manager.verify(speakerName, embedding, threshold)
            
            if (verified) {
                println("验证成功: $speakerName")
            } else {
                println("验证失败: 相似度低于阈值")
            }
            
            return verified
        }
        
        return false
    } finally {
        stream.release()
    }
}

// 使用示例
fun exampleIdentify() {
    // 假设已录制待识别的音频数据
    val audioSamples = listOf(
        FloatArray(16000)  // 1秒的音频（16kHz采样率）
    )
    
    // 识别说话人
    val speakerName = identifySpeaker(audioSamples, 16000, 0.5f)
    
    if (speakerName != null) {
        println("识别到的说话人: $speakerName")
    } else {
        println("未识别到说话人")
    }
    
    // 验证说话人身份
    if (speakerName != null) {
        val verified = verifySpeaker(speakerName, audioSamples, 16000, 0.5f)
        println("验证结果: $verified")
    }
}
```

## 六、完整使用示例

以下是一个完整的使用示例，展示如何初始化和使用说话人识别功能：

```kotlin
import android.content.res.AssetManager
import com.k2fsa.sherpa.onnx.*

class SpeakerRecognitionExample {
    
    // 初始化
    fun initialize(assetManager: AssetManager?) {
        SpeakerRecognition.initExtractor(assetManager)
        println("说话人识别系统初始化完成")
        println("特征向量维度: ${SpeakerRecognition.extractor.dim()}")
    }
    
    // 注册多个说话人
    fun registerMultipleSpeakers() {
        val manager = SpeakerRecognition.manager
        
        // 注册说话人1
        val speaker1Samples = listOf(
            FloatArray(1600),
            FloatArray(1600),
            FloatArray(1600)
        )
        registerSpeaker("Alice", speaker1Samples, 16000)
        
        // 注册说话人2
        val speaker2Samples = listOf(
            FloatArray(1600),
            FloatArray(1600)
        )
        registerSpeaker("Bob", speaker2Samples, 16000)
        
        // 查看所有已注册的说话人
        val allSpeakers = manager.allSpeakerNames()
        println("已注册的说话人: ${allSpeakers.joinToString(", ")}")
        println("说话人总数: ${manager.numSpeakers()}")
    }
    
    // 识别说话人
    fun identifyExample() {
        val testSamples = listOf(FloatArray(16000))
        val result = identifySpeaker(testSamples, 16000, 0.5f)
        
        if (result != null) {
            println("识别成功: $result")
        }
    }
    
    // 管理说话人
    fun manageSpeakers() {
        val manager = SpeakerRecognition.manager
        
        // 检查说话人是否存在
        if (manager.contains("Alice")) {
            println("Alice 已注册")
        }
        
        // 移除说话人
        if (manager.remove("Bob")) {
            println("已移除 Bob")
        }
        
        // 查看剩余说话人
        println("剩余说话人数量: ${manager.numSpeakers()}")
    }
    
    // 清理资源（在应用退出时调用）
    fun cleanup() {
        SpeakerRecognition.extractor.release()
        SpeakerRecognition.manager.release()
    }
}
```

## 七、注意事项

1. **音频格式要求**：
   - 采样率：16kHz
   - 声道：单声道（Mono）
   - 格式：PCM 16-bit，转换为 FloatArray（值范围 -1.0 到 1.0）

2. **特征向量维度**：
   - 使用 `SpeakerEmbeddingExtractor.dim()` 获取特征向量维度
   - 创建 `SpeakerEmbeddingManager` 时必须使用相同的维度

3. **相似度阈值**：
   - 阈值范围通常为 0.0-1.0
   - 阈值越高，识别越严格（减少误识别，但可能增加拒识）
   - 阈值越低，识别越宽松（减少拒识，但可能增加误识别）
   - 建议根据实际场景调整阈值（通常 0.5-0.7 之间）

4. **资源管理**：
   - 使用完 `OnlineStream` 后应调用 `release()` 释放资源
   - 可以使用 `use { }` 块自动管理资源
   - 应用退出时应释放 `SpeakerEmbeddingExtractor` 和 `SpeakerEmbeddingManager`

5. **多段音频注册**：
   - 建议为每个说话人录制多段音频（3-5段）以提高识别准确率
   - 使用 `add(name, Array<FloatArray>)` 方法可以一次性添加多个特征向量

6. **线程安全**：
   - `SpeakerRecognition.initExtractor()` 内部已做同步处理
   - 其他操作建议在单线程中执行，或自行添加同步机制

