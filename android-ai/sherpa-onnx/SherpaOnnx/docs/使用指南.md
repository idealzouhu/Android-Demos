# Sherpa-ONNX Android 使用指南

## 一、ASR 原理简介

自动语音识别（ASR，Automatic Speech Recognition）是将人类语音转换为文本的技术。本项目使用的 Sherpa-ONNX 是一个基于 ONNX（Open Neural Network Exchange）的流式语音识别框架。

### 1.1 工作原理

1. **音频采集**：通过 Android 的 `AudioRecord` API 实时采集麦克风音频数据
2. **特征提取**：将原始音频信号转换为特征向量（如 Mel 频谱特征）
3. **模型推理**：使用预训练的神经网络模型（支持多种架构：Zipformer、LSTM、Paraformer 等）进行声学建模
4. **解码**：将模型输出的概率分布转换为文本序列
5. **流式处理**：支持实时流式识别，可以边录音边识别，无需等待完整音频

### 1.2 支持的模型架构

- **Transducer 模型**：包含 encoder、decoder 和 joiner 三个组件
- **Paraformer 模型**：基于 Paraformer 架构的流式识别模型
- **CTC 模型**：支持 Zipformer2 CTC、NeMo CTC、Tone CTC 等

## 二、SO 库和模型文件

### 2.1 SO 库介绍

项目使用以下 JNI 库（位于 `app/src/main/jniLibs/` 目录）：

- **libsherpa-onnx-jni.so**：SherpaOnnx 的 JNI 接口库，提供 Kotlin/Java 与底层 C++ 代码的桥接
- **libonnxruntime.so**：ONNX Runtime 库，用于执行 ONNX 模型推理

这些库支持以下架构：

- `arm64-v8a`：64位 ARM 架构（现代 Android 设备）
- `armeabi-v7a`：32位 ARM 架构（较旧的 Android 设备）
- `x86`、`x86_64`：x86 架构（模拟器）

下载链接为 https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.23/sherpa-onnx-v1.12.23-android.tar.bz2

### 2.2 模型文件

#### 2.2.1 下载模型文件

模型文件需要放置在 `app/src/main/assets/` 目录下。项目支持多种不同语言、不同模型架构、流式和非流式的预训练模型，包括：
- `sherpa-onnx-lstm-zh-2023-02-20`：中文 LSTM 模型
- `sherpa-onnx-streaming-zipformer-zh-14M-2023-02-23`：中文 Zipformer 模型（14M 参数）
- `sherpa-onnx-streaming-zipformer-ctc-zh-2025-06-30`：中文 Zipformer CTC 模型
- `sherpa-onnx-lstm-en-2023-02-17`：英文 LSTM 模型
- `sherpa-onnx-streaming-zipformer-en-2023-06-26`：英文 Zipformer 模型
- `sherpa-onnx-nemo-streaming-fast-conformer-ctc-en-80ms`：英文 NeMo CTC 模型
- `sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20`：中英双语 Zipformer 模型
- `sherpa-onnx-streaming-paraformer-bilingual-zh-en`：中英双语 Paraformer 模型
> 更多预训练模型请参考 [官方预训练模型列表](https://k2-fsa.github.io/sherpa/onnx/pretrained_models/index.html)。

解析下载的压缩包 [sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20.tar.bz2](https://github.com/k2-fsa/sherpa-onnx/releases/download/asr-models/sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20.tar.bz2) ，我们可以看到如下内容：

```Bash
$ ls -lh
total 530M
-rw-r--r-- 1 zouhu 197121  296 Feb  3 14:18 README.md
-rw-r--r-- 1 zouhu 197121 240K May 28  2024 bpe.model
-rw-r--r-- 1 zouhu 197121  13K May 28  2024 bpe.vocab
-rw-r--r-- 1 zouhu 197121  13M May 28  2024 decoder-epoch-99-avg-1.int8.onnx
-rw-r--r-- 1 zouhu 197121  14M May 28  2024 decoder-epoch-99-avg-1.onnx
-rw-r--r-- 1 zouhu 197121 174M May 28  2024 encoder-epoch-99-avg-1.int8.onnx
-rw-r--r-- 1 zouhu 197121 315M May 28  2024 encoder-epoch-99-avg-1.onnx
-rw-r--r-- 1 zouhu 197121 3.1M May 28  2024 joiner-epoch-99-avg-1.int8.onnx
-rw-r--r-- 1 zouhu 197121  13M May 28  2024 joiner-epoch-99-avg-1.onnx
drwxr-xr-x 1 zouhu 197121    0 May 28  2024 test_wavs/
-rw-r--r-- 1 zouhu 197121  55K May 28  2024 tokens.txt
```


### 2.2.2 模型文件结构

每个模型目录通常包含以下文件：

**Transducer 模型**：
- `encoder-*.onnx`：编码器模型
- `decoder-*.onnx`：解码器模型
- `joiner-*.onnx`：连接器模型
- `tokens.txt`：词汇表文件

**Paraformer 模型**：
- `encoder-*.onnx`：编码器模型
- `decoder-*.onnx`：解码器模型
- `tokens.txt`：词汇表文件

**CTC 模型**：
- `model.onnx` 或 `model.int8.onnx`：单文件模型
- `tokens.txt`：词汇表文件

**可选文件**：
- `with-state-*.onnx`：语言模型（用于提升识别准确率）
- `lexicon.txt`：词典文件（用于同音字替换）
- `replace.fst`：FST 规则文件（用于同音字替换）


## 三、SO 库接口和主要组件

### 3.1 主要类介绍

#### 3.1.1 OnlineRecognizer

核心识别器类，负责初始化模型和执行识别任务。

**主要方法**：
- `createStream(hotwords: String = "")`：创建一个识别流，用于处理音频数据
- `decode(stream: OnlineStream)`：对音频流进行解码
- `isReady(stream: OnlineStream)`：检查流是否准备好进行解码
- `isEndpoint(stream: OnlineStream)`：检查是否到达端点（检测到静音）
- `getResult(stream: OnlineStream)`：获取识别结果，返回 `OnlineRecognizerResult` 对象
- `reset(stream: OnlineStream)`：重置流状态
- `release()`：释放资源

#### 3.1.2 OnlineStream

音频流处理类，用于接收和处理音频数据。

**主要方法**：
- `acceptWaveform(samples: FloatArray, sampleRate: Int)`：接收音频波形数据
- `inputFinished()`：标记输入完成
- `release()`：释放流资源

#### 3.1.3 OnlineRecognizerResult

识别结果类，包含识别输出的所有信息。

**主要属性**：
- `text: String`：识别出的文本内容
- `tokens: Array<String>`：识别出的词元（token）数组
- `timestamps: FloatArray`：每个词元对应的时间戳数组
- `ysProbs: FloatArray`：每个词元的概率数组

### 3.2 配置类说明

所有配置类都是 `OnlineRecognizerConfig` 的参数，用于构建识别器配置。

#### 3.2.1 OnlineRecognizerConfig

识别器主配置类，包含所有配置参数。创建 `OnlineRecognizer` 时必须提供此配置对象。

**主要配置项**：
- `featConfig: FeatureConfig`：特征提取配置（见 3.2.2）
- `modelConfig: OnlineModelConfig`：模型配置（见 3.2.3）
- `lmConfig: OnlineLMConfig`：语言模型配置（见 3.2.4，可选）
- `ctcFstDecoderConfig: OnlineCtcFstDecoderConfig`：CTC FST 解码器配置（见 3.2.5，可选）
- `hr: HomophoneReplacerConfig`：同音字替换配置（见 3.2.6，可选）
- `endpointConfig: EndpointConfig`：端点检测配置（见 3.2.7）
- `enableEndpoint: Boolean`：是否启用端点检测（默认 `true`）
- `decodingMethod: String`：解码方法（如 `"greedy_search"`，默认 `"greedy_search"`）
- `maxActivePaths: Int`：最大活跃路径数（默认 `4`）
- `hotwordsFile: String`：热词文件路径（可选）
- `hotwordsScore: Float`：热词权重（默认 `1.5f`）
- `ruleFsts: String`：FST 规则文件路径（可选）
- `ruleFars: String`：FAR 规则文件路径（可选）
- `blankPenalty: Float`：空白符惩罚值（默认 `0.0f`）

#### 3.2.2 FeatureConfig

特征提取配置类，作为 `OnlineRecognizerConfig.featConfig` 使用。

**配置项**：
- `sampleRate: Int`：采样率（通常为 `16000` Hz，默认 `16000`）
- `featureDim: Int`：特征维度（通常为 `80`，默认 `80`）
- `dither: Float`：抖动参数（默认 `0.0f`）

#### 3.2.3 OnlineModelConfig

模型配置类，作为 `OnlineRecognizerConfig.modelConfig` 使用。指定使用的模型类型和文件路径。

**主要配置项**：
- `transducer: OnlineTransducerModelConfig`：Transducer 模型配置
- `paraformer: OnlineParaformerModelConfig`：Paraformer 模型配置（
- `zipformer2Ctc: OnlineZipformer2CtcModelConfig`：Zipformer2 CTC 模型配置
- `neMoCtc: OnlineNeMoCtcModelConfig`：NeMo CTC 模型配置
- `toneCtc: OnlineToneCtcModelConfig`：Tone CTC 模型配置
- `tokens: String`：词汇表文件路径（必需）
- `modelType: String`：模型类型（如 `"zipformer"`, `"lstm"`, `"paraformer"` 等）
- `modelingUnit: String`：建模单元（如 `"bpe"`, `"char"` 等）
- `bpeVocab: String`：BPE 词汇表文件路径（当 `modelingUnit = "bpe"` 时使用）
- `numThreads: Int`：推理线程数（默认 `1`）
- `debug: Boolean`：是否启用调试模式（默认 `false`）
- `provider: String`：执行提供者（如 `"cpu"`, `"rknn"` 等，默认 `"cpu"`）

**注意**：根据模型类型，只需配置对应的模型配置项（transducer、paraformer、zipformer2Ctc 等其中之一）。

#### 3.2.4 HomophoneReplacerConfig

同音字替换配置类，作为 `OnlineRecognizerConfig.hr` 使用（可选）。用于替换识别结果中的同音字。

**配置项**：
- `lexicon: String`：词典文件路径（`lexicon.txt`）
- `ruleFsts: String`：FST 规则文件路径（`replace.fst`）
- `dictDir: String`：词典目录（未使用）

#### 3.2.5 EndpointConfig

端点检测配置类，作为 `OnlineRecognizerConfig.endpointConfig` 使用。用于检测语音结束。

**配置规则**：
- `rule1: EndpointRule`：规则1
  - `mustContainNonSilence: Boolean`：是否必须包含非静音（默认 `false`）
  - `minTrailingSilence: Float`：最小尾随静音时间（秒，默认 `2.4f`）
  - `minUtteranceLength: Float`：最小话语长度（秒，默认 `0.0f`）
- `rule2: EndpointRule`：规则2（默认：`mustContainNonSilence = true`, `minTrailingSilence = 1.4f`, `minUtteranceLength = 0.0f`）
- `rule3: EndpointRule`：规则3（默认：`mustContainNonSilence = false`, `minTrailingSilence = 0.0f`, `minUtteranceLength = 20.0f`）

**说明**：当满足任一规则时，即判定为到达端点。


## 四、简单实现案例

### 4.1 使用流程

1. **初始化配置**：创建 `OnlineRecognizerConfig` 对象，配置模型路径和参数
2. **创建识别器**：使用配置创建 `OnlineRecognizer` 实例
3. **创建流**：调用 `createStream()` 创建音频流
4. **输入音频**：通过 `acceptWaveform()` 输入音频数据
5. **解码**：调用 `decode()` 进行识别
6. **获取结果**：通过 `getResult()` 获取识别文本
7. **重置/释放**：识别完成后调用 `reset()` 或 `release()` 释放资源

### 4.2 完整代码示例

以下是一个完整的流式语音识别示例：

```kotlin
package com.k2fsa.sherpa.onnx

import android.Manifest
import android.content.pm.PackageManager
import android.media.AudioFormat
import android.media.AudioRecord
import android.media.MediaRecorder
import android.os.Bundle
import android.util.Log
import android.widget.Button
import android.widget.TextView
import androidx.appcompat.app.AppCompatActivity
import androidx.core.app.ActivityCompat
import kotlin.concurrent.thread

class MainActivity : AppCompatActivity() {
    private lateinit var recognizer: OnlineRecognizer
    private var audioRecord: AudioRecord? = null
    private lateinit var recordButton: Button
    private lateinit var textView: TextView
    private var recordingThread: Thread? = null
    
    private val sampleRateInHz = 16000
    private val channelConfig = AudioFormat.CHANNEL_IN_MONO
    private val audioFormat = AudioFormat.ENCODING_PCM_16BIT
    
    @Volatile
    private var isRecording: Boolean = false

    override fun onCreate(savedInstanceState: Bundle?) {
        super.onCreate(savedInstanceState)
        setContentView(R.layout.activity_main)
        
        // 请求录音权限
        ActivityCompat.requestPermissions(
            this, 
            arrayOf(Manifest.permission.RECORD_AUDIO), 
            200
        )
        
        // 初始化模型
        initModel()
        
        recordButton = findViewById(R.id.record_button)
        recordButton.setOnClickListener { toggleRecording() }
        
        textView = findViewById(R.id.my_text)
    }
    
    /**
     * 初始化识别器模型
     */
    private fun initModel() {
        // 配置特征提取
        val featConfig = FeatureConfig(
            sampleRate = sampleRateInHz,
            featureDim = 80
        )
        
        // 配置模型（使用类型 0：中英双语模型）
        val modelConfig = OnlineModelConfig(
            transducer = OnlineTransducerModelConfig(
                encoder = "sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20/encoder-epoch-99-avg-1.onnx",
                decoder = "sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20/decoder-epoch-99-avg-1.onnx",
                joiner = "sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20/joiner-epoch-99-avg-1.onnx",
            ),
            tokens = "sherpa-onnx-streaming-zipformer-bilingual-zh-en-2023-02-20/tokens.txt",
            modelType = "zipformer",
            numThreads = 1,
            provider = "cpu"
        )
        
        // 配置端点检测
        val endpointConfig = EndpointConfig(
            rule1 = EndpointRule(false, 2.4f, 0.0f),
            rule2 = EndpointRule(true, 1.4f, 0.0f),
            rule3 = EndpointRule(false, 0.0f, 20.0f)
        )
        
        // 创建识别器配置
        val config = OnlineRecognizerConfig(
            featConfig = featConfig,
            modelConfig = modelConfig,
            endpointConfig = endpointConfig,
            enableEndpoint = true,
            decodingMethod = "greedy_search"
        )
        
        // 创建识别器实例（从 assets 加载模型）
        recognizer = OnlineRecognizer(
            assetManager = application.assets,
            config = config
        )
    }
    
    /**
     * 切换录音状态
     */
    private fun toggleRecording() {
        if (!isRecording) {
            startRecording()
        } else {
            stopRecording()
        }
    }
    
    /**
     * 开始录音和识别
     */
    private fun startRecording() {
        if (ActivityCompat.checkSelfPermission(
                this, Manifest.permission.RECORD_AUDIO
            ) != PackageManager.PERMISSION_GRANTED
        ) {
            return
        }
        
        val bufferSize = AudioRecord.getMinBufferSize(
            sampleRateInHz, 
            channelConfig, 
            audioFormat
        )
        
        audioRecord = AudioRecord(
            MediaRecorder.AudioSource.MIC,
            sampleRateInHz,
            channelConfig,
            audioFormat,
            bufferSize * 2
        )
        
        audioRecord?.startRecording()
        recordButton.text = "停止"
        isRecording = true
        textView.text = ""
        
        // 在后台线程处理音频
        recordingThread = thread {
            processAudio()
        }
    }
    
    /**
     * 停止录音
     */
    private fun stopRecording() {
        isRecording = false
        audioRecord?.stop()
        audioRecord?.release()
        audioRecord = null
        recordButton.text = "开始"
    }
    
    /**
     * 处理音频数据
     */
    private fun processAudio() {
        // 创建识别流
        val stream = recognizer.createStream()
        
        // 每次处理 100ms 的音频
        val interval = 0.1
        val bufferSize = (interval * sampleRateInHz).toInt()
        val buffer = ShortArray(bufferSize)
        
        try {
            while (isRecording) {
                val ret = audioRecord?.read(buffer, 0, buffer.size) ?: 0
                
                if (ret > 0) {
                    // 将 16 位 PCM 转换为浮点数（-1.0 到 1.0）
                    val samples = FloatArray(ret) { buffer[it] / 32768.0f }
                    
                    // 输入音频数据到流
                    stream.acceptWaveform(samples, sampleRate = sampleRateInHz)
                    
                    // 当流准备好时进行解码
                    while (recognizer.isReady(stream)) {
                        recognizer.decode(stream)
                    }
                    
                    // 检查是否到达端点（检测到静音）
                    val isEndpoint = recognizer.isEndpoint(stream)
                    
                    // 获取识别结果
                    val result = recognizer.getResult(stream)
                    val text = result.text
                    
                    // 更新 UI
                    if (text.isNotBlank()) {
                        runOnUiThread {
                            textView.text = text
                        }
                    }
                    
                    // 如果到达端点，重置流
                    if (isEndpoint) {
                        recognizer.reset(stream)
                    }
                }
            }
        } finally {
            // 释放流资源
            stream.release()
        }
    }
}
```

### 4.3 关键步骤说明

1. **权限申请**：在 `onCreate()` 中申请 `RECORD_AUDIO` 权限
2. **模型初始化**：在 `initModel()` 中配置并创建 `OnlineRecognizer`
3. **音频采集**：使用 `AudioRecord` 采集音频数据
4. **流式处理**：
   - 创建 `OnlineStream`
   - 通过 `acceptWaveform()` 输入音频
   - 调用 `decode()` 进行识别
   - 使用 `getResult()` 获取结果
   - 通过 `isEndpoint()` 检测语音结束
5. **资源释放**：录音结束后释放 `AudioRecord` 和 `OnlineStream`

### 4.4 注意事项

1. **模型文件**：确保模型文件已放置在 `app/src/main/assets/` 目录下
2. **采样率**：模型通常要求 16kHz 采样率，确保 `AudioRecord` 配置正确
3. **线程处理**：音频处理应在后台线程进行，避免阻塞 UI 线程
4. **资源管理**：及时释放 `AudioRecord` 和 `OnlineStream` 资源
5. **端点检测**：合理配置 `EndpointConfig` 以准确检测语音结束
6. **模型选择**：根据应用场景选择合适的模型（中文、英文或双语）

### 4.5 扩展功能

- **热词支持**：通过 `hotwordsFile` 配置热词文件，提升特定词汇识别率
- **语言模型**：通过 `lmConfig` 配置语言模型，提升识别准确率
- **同音字替换**：通过 `HomophoneReplacerConfig` 配置同音字替换规则
- **多线程推理**：通过 `numThreads` 配置推理线程数，提升性能

## 参考资料
- 官方文档：https://k2-fsa.github.io/sherpa/onnx/
- GitHub 仓库：https://github.com/k2-fsa/sherpa-onnx
