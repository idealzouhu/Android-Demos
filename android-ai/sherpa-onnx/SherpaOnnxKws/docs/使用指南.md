# SherpaOnnxKws 使用指南

## 一、KWS 原理简介

关键词检测（Keyword Spotting, KWS）是一种在连续语音流中实时检测特定关键词或短语的技术。本项目实现的是**开放词汇关键词检测**（Open Vocabulary Keyword Spotting），这是一种无需重新训练模型即可检测任意关键词的先进方法。

### 1.1 工作原理

开放词汇关键词检测系统本质上是一个**小型自动语音识别（ASR）系统**，但它只能解码给定的关键词。例如，如果给定的关键词是 `HELLO WORLD`，那么解码结果只能是 `HELLO WORLD` 或空（未检测到）。

与传统关键词检测系统相比，开放词汇关键词检测的优势在于：

- **无需重新训练**：使用一个预训练模型即可检测不同的关键词，即使这些关键词不在训练数据中
- **灵活配置**：可以动态指定要检测的关键词
- **平衡性能**：通过 `boosting score`（提升分数）和 `trigger threshold`（触发阈值）两个参数来平衡触发率和误报率
  - **boosting score**：帮助包含关键词的路径在 beam search 中存活，值越大越容易触发
  - **trigger threshold**：定义可触发序列的最小声学概率（0-1之间），值越小越容易触发



### 1.2 工作流程

```
┌──────────────────────────────────────────────────────────────┐
│                      完整处理流程                              │
├──────────────────────────────────────────────────────────────┤
│                                                               │
│  1. 音频输入 (PCM 16-bit, 16kHz)                             │
│     ↓                                                         │
│  2. 特征提取 (MFCC/FBank, 80维)                              │
│     ↓                                                         │
│  3. Encoder 推理 (ONNX)                                      │
│     输出: encoder_output [T, encoder_dim]                    │
│     ↓                                                         │
│  4. Decoder 推理 (ONNX) - 基于当前路径状态                    │
│     输出: decoder_output [decoder_dim]                       │
│     ↓                                                         │
│  5. Joiner 推理 (ONNX)                                       │
│     输出: logits [T, vocab_size]                             │
│     ↓                                                         │
│  6. Softmax 归一化                                           │
│     输出: probs [T, vocab_size] ← 模型的最终输出             │
│     ↓                                                         │
│  7. Beam Search 解码 (C++ 实现)                              │
│     输入: probs [T, vocab_size]                              │
│     处理: 构建路径、应用约束、选择 top-K                     │
│     输出: KeywordSpotterResult {                             │
│              keyword: "HELLO WORLD",                         │
│              tokens: ["HELLO", "WORLD"],                     │
│              timestamps: [0.5, 1.2]                          │
│           }                                                   │
│                                                               │
└──────────────────────────────────────────────────────────────┘
```





### 1.3 KWS 和 ASR 的区别

**KWS 和 ASR 的模型输出在形式上确实非常相似**，它们通常都输出一个概率分布。但它们的**语义（Semantic）和粒度（Granularity）完全不同**，这直接决定了它们的技术路线和应用场景。

| 维度         | **ASR (语音识别)**                           | **KWS (关键词检测)**                                   |
| :----------- | :------------------------------------------- | :----------------------------------------------------- |
| **输出维度** | **词汇表 (Vocabulary)**                      | **关键词集合 (Keyword Set)**                           |
| **语义**     | 下一个词元是“苹果”、“香蕉”还是“你好”的概率。 | 当前帧是否包含“关键词A”、“关键词B”或“非关键词”的概率。 |
| **粒度**     | **细粒度**：需要区分成千上万个词。           | **粗粒度**：通常只区分几个关键词和“其他”。             |







## 二、SO 库和模型文件

### 2.1 SO 库介绍

项目使用以下 JNI 库（位于 `app/src/main/jniLibs/` 目录）：

- **libsherpa-onnx-jni.so**：SherpaOnnx 的 JNI 接口库，提供 Kotlin/Java 与底层 C++ 代码的桥接
- **libonnxruntime.so**：ONNX Runtime 库，用于执行 ONNX 模型推理

这些库支持以下架构：
- `arm64-v8a`：64位 ARM 架构（现代 Android 设备）
- `armeabi-v7a`：32位 ARM 架构（较旧的 Android 设备）
- `x86`、`x86_64`：x86 架构（模拟器）

下载链接为 https://github.com/k2-fsa/sherpa-onnx/releases/download/v1.12.23/sherpa-onnx-v1.12.23-android.tar.bz2



### 2.2 模型文件

2.2.1 下载模型文件

项目支持以下预训练模型：
- [**sherpa-onnx-kws-zipformer-zh-en-3M-2025-12-20 (Chinese & English)**](https://github.com/k2-fsa/sherpa-onnx/releases/download/kws-models/sherpa-onnx-kws-zipformer-zh-en-3M-2025-12-20.tar.bz2)
- [**sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01 (Chinese)**](https://github.com/k2-fsa/sherpa-onnx/releases/download/kws-models/sherpa-onnx-kws-zipformer-wenetspeech-3.3M-2024-01-01.tar.bz2)
- [**sherpa-onnx-kws-zipformer-gigaspeech-3.3M-2024-01-01 (English)**](https://github.com/k2-fsa/sherpa-onnx/releases/download/kws-models/sherpa-onnx-kws-zipformer-gigaspeech-3.3M-2024-01-01.tar.bz2)

更多预训练模型请参考 [官方预训练模型列表](https://k2-fsa.github.io/sherpa/onnx/kws/pretrained_models/index.html)。解析下载的压缩包，我们可以看到如下内容：

``` Bash
$ ls -lh
total 38M
-rw-r--r-- 1 Zouhu 197121 743K Dec 22 17:14 decoder-epoch-13-avg-2-chunk-16-left-64.onnx
-rw-r--r-- 1 Zouhu 197121 743K Dec 22 17:14 decoder-epoch-13-avg-2-chunk-8-left-64.onnx
-rw-r--r-- 1 Zouhu 197121 3.2M Dec 22 17:15 en.phone
-rw-r--r-- 1 Zouhu 197121 4.4M Dec 22 17:14 encoder-epoch-13-avg-2-chunk-16-left-64.int8.onnx
-rw-r--r-- 1 Zouhu 197121  12M Dec 22 17:14 encoder-epoch-13-avg-2-chunk-16-left-64.onnx
-rw-r--r-- 1 Zouhu 197121 4.4M Dec 22 17:14 encoder-epoch-13-avg-2-chunk-8-left-64.int8.onnx
-rw-r--r-- 1 Zouhu 197121  12M Dec 22 17:14 encoder-epoch-13-avg-2-chunk-8-left-64.onnx
-rw-r--r-- 1 Zouhu 197121  85K Dec 22 17:14 joiner-epoch-13-avg-2-chunk-16-left-64.int8.onnx
-rw-r--r-- 1 Zouhu 197121 331K Dec 22 17:14 joiner-epoch-13-avg-2-chunk-16-left-64.onnx
-rw-r--r-- 1 Zouhu 197121  85K Dec 22 17:14 joiner-epoch-13-avg-2-chunk-8-left-64.int8.onnx
-rw-r--r-- 1 Zouhu 197121 331K Dec 22 17:14 joiner-epoch-13-avg-2-chunk-8-left-64.onnx
drwxr-xr-x 1 Zouhu 197121    0 Dec 22 18:46 test_wavs/
-rw-r--r-- 1 Zouhu 197121 1.9K Dec 22 17:15 tokens.txt
```


#### 2.2.1 模型文件说明

下载解压后的模型目录包含以下文件：

**核心模型文件：**
- `encoder-*.onnx`：编码器模型（处理音频特征）
- `decoder-*.onnx`：解码器模型（处理历史词元序列）
- `joiner-*.onnx`：连接器模型（合并编码器和解码器输出）
- `tokens.txt`：词汇表文件（必需，用于 Token ID 到词元的映射）

**模型版本选择：**

1. **Chunk Size 版本**（影响延迟）：
   - `chunk-8`：延迟 160ms，适合对实时性要求高的场景
   - `chunk-16`：延迟 320ms，准确率通常更高
   - **建议**：根据应用需求选择，低延迟通常意味着准确率略低

2. **精度版本**（影响模型大小和速度）：
   - `fp32`：32位浮点，准确率最高，模型较大
   - `int8`：8位整数，模型更小、推理更快，准确率略低
   - **注意**：decoder 量化收益不大，通常只提供 fp32 版本。可以混合使用：int8 encoder + int8 joiner + fp32 decoder

**使用建议：**
- 移动设备：优先选择 `chunk-8` + `int8` 版本（平衡速度和准确率）
- 对准确率要求高：选择 `chunk-16` + `fp32` 版本
- 需要最小模型体积：选择 `int8` 版本



### 2.3 模型文件结构

每个模型包含以下文件（需要放置在 `app/src/main/assets/` 目录下）：

**必需文件：**
```
模型目录名/
├── encoder-*.onnx          # 编码器模型（选择 chunk-8 或 chunk-16，fp32 或 int8）
├── decoder-*.onnx          # 解码器模型（通常只有 fp32 版本）
├── joiner-*.onnx           # 连接器模型（选择与 encoder 对应的版本）
└── tokens.txt              # 词汇表文件（必需）
```

**可选文件：**
- `keywords.txt`：默认关键词文件
- `*.phone`：音素文件（某些模型需要）
- `test_wavs/`：测试音频目录

**注意：** 确保 encoder、decoder、joiner 的 chunk size 版本一致（都是 chunk-8 或都是 chunk-16）。



## 三、API 接口参考

### 3.1 核心组件

#### 3.1.1 KeywordSpotter - 关键词检测器

主要的关键词检测类，负责初始化模型和执行检测。

**主要方法：**

```kotlin
// 创建音频流
fun createStream(keywords: String = ""): OnlineStream

// 解码音频流
fun decode(stream: OnlineStream)

// 检查是否准备好解码
fun isReady(stream: OnlineStream): Boolean

// 获取检测结果
fun getResult(stream: OnlineStream): KeywordSpotterResult

// 重置流状态（检测到关键词后应调用）
fun reset(stream: OnlineStream)

// 释放资源
fun release()
```

**初始化方式：**

```kotlin
// 从 Assets 加载模型
val kws = KeywordSpotter(
    assetManager = application.assets,
    config = config
)

// 从文件系统加载模型
val kws = KeywordSpotter(
    assetManager = null,
    config = config
)
```

#### 3.1.2 OnlineStream - 音频流处理

用于处理实时音频流的类。

**主要方法：**

```kotlin
// 输入音频波形数据
fun acceptWaveform(samples: FloatArray, sampleRate: Int)

// 标记输入结束
fun inputFinished()

// 释放资源
fun release()

// 使用后自动释放（Kotlin 扩展函数）
fun use(block: (OnlineStream) -> Unit)
```

#### 3.1.3 KeywordSpotterResult - 检测结果

检测结果的数据结构。

```kotlin
data class KeywordSpotterResult(
    val keyword: String,              // 检测到的关键词
    val tokens: Array<String>,        // 词元序列
    val timestamps: FloatArray,       // 时间戳数组
)
```

### 3.2 配置类

#### 3.2.1 KeywordSpotterConfig - 检测器配置

关键词检测器的完整配置参数。

```kotlin
data class KeywordSpotterConfig(
    var featConfig: FeatureConfig = FeatureConfig(),        // 特征提取配置
    var modelConfig: OnlineModelConfig = OnlineModelConfig(), // 模型配置
    var maxActivePaths: Int = 4,                            // Beam search 最大活跃路径数
    var keywordsFile: String = "keywords.txt",             // 关键词文件路径
    var keywordsScore: Float = 1.5f,                       // 关键词提升分数
    var keywordsThreshold: Float = 0.25f,                  // 触发阈值
    var numTrailingBlanks: Int = 2,                        // 尾随空白数量
)
```

#### 3.2.2 FeatureConfig - 特征提取配置

音频特征提取的配置参数。

```kotlin
data class FeatureConfig(
    var sampleRate: Int = 16000,      // 采样率（Hz）
    var featureDim: Int = 80,         // 特征维度
    var dither: Float = 0.0f          // 抖动系数
)
```

#### 3.2.3 OnlineModelConfig - 模型配置

ONNX 模型的配置信息。

```kotlin
data class OnlineModelConfig(
    var transducer: OnlineTransducerModelConfig = OnlineTransducerModelConfig(),
    var tokens: String = "",          // tokens.txt 文件路径
    var modelType: String = "",       // 模型类型（如 "zipformer2"）
    var numThreads: Int = 1,          // 推理线程数
    var provider: String = "cpu",     // 执行提供者（cpu, cuda 等）
    var debug: Boolean = false,       // 调试模式
)
```



## 四、实现案例

### 4.1 基本使用流程

以下是一个完整的关键词检测实现示例：

```kotlin
import com.k2fsa.sherpa.onnx.*
import android.media.AudioRecord
import android.media.MediaRecorder
import android.media.AudioFormat

class KeywordSpottingExample {
    private lateinit var kws: KeywordSpotter
    private lateinit var stream: OnlineStream
    private var audioRecord: AudioRecord? = null
    
    private val sampleRateInHz = 16000
    private val channelConfig = AudioFormat.CHANNEL_IN_MONO
    private val audioFormat = AudioFormat.ENCODING_PCM_16BIT
    
    // 1. 初始化模型
    fun initModel(assetManager: android.content.res.AssetManager) {
        val config = KeywordSpotterConfig(
            featConfig = getFeatureConfig(
                sampleRate = sampleRateInHz,
                featureDim = 80
            ),
            modelConfig = getKwsModelConfig(type = 0)!!, 
            keywordsFile = getKeywordsFile(type = 0),
            keywordsScore = 1.5f,      // 提升分数
            keywordsThreshold = 0.25f, // 触发阈值
        )
        
        kws = KeywordSpotter(
            assetManager = assetManager,
            config = config
        )
    }
    
    // 2. 开始检测
    fun startDetection(keywords: String) {
        // 创建音频流，可以传入自定义关键词
        stream = kws.createStream(keywords)
        
        // 初始化麦克风
        val bufferSize = AudioRecord.getMinBufferSize(
            sampleRateInHz,
            channelConfig,
            audioFormat
        )
        
        audioRecord = AudioRecord(
            MediaRecorder.AudioSource.MIC,
            sampleRateInHz,
            channelConfig,
            audioFormat,
            bufferSize * 2
        )
        
        audioRecord?.startRecording()
        
        // 在后台线程处理音频
        Thread {
            processAudio()
        }.start()
    }
    
    // 3. 处理音频数据
    private fun processAudio() {
        val interval = 0.1 // 100ms
        val bufferSize = (interval * sampleRateInHz).toInt()
        val buffer = ShortArray(bufferSize)
        
        while (isRecording) {
            val ret = audioRecord?.read(buffer, 0, buffer.size) ?: 0
            
            if (ret > 0) {
                // 转换为浮点数组（归一化到 -1.0 到 1.0）
                val samples = FloatArray(ret) { buffer[it] / 32768.0f }
                
                // 输入音频数据到流
                stream.acceptWaveform(samples, sampleRate = sampleRateInHz)
                
                // 循环解码直到没有更多结果
                while (kws.isReady(stream)) {
                    kws.decode(stream)
                    
                    val result = kws.getResult(stream)
                    
                    // 检测到关键词
                    if (result.keyword.isNotBlank()) {
                        println("检测到关键词: ${result.keyword}")
                        println("词元: ${result.tokens.joinToString()}")
                        println("时间戳: ${result.timestamps.joinToString()}")
                        
                        // 重要：检测到关键词后重置流
                        kws.reset(stream)
                    }
                }
            }
        }
        
        // 清理资源
        stream.release()
        audioRecord?.stop()
        audioRecord?.release()
        audioRecord = null
    }
    
    // 4. 停止检测
    fun stopDetection() {
        isRecording = false
        audioRecord?.stop()
        audioRecord?.release()
        stream.release()
    }
    
    // 5. 释放资源
    fun release() {
        kws.release()
    }
}
```

### 4.2 自定义关键词

#### 方法一：通过代码动态设置

```kotlin
// 创建流时传入关键词（多个关键词用 "/" 分隔）
val keywords = "小爱同学/你好问问/小艺小艺"
stream = kws.createStream(keywords)
```

#### 方法二：使用关键词文件

关键词文件格式（`keywords.txt`）：

```
▁HE LL O ▁WORLD :1.5 #0.35
▁HI ▁GO O G LE :1.0 #0.25
▁HE Y ▁S I RI
```

格式说明：
- 每行一个关键词
- 词元之间用空格分隔
- `:1.5` 表示 boosting score 为 1.5
- `#0.35` 表示 trigger threshold 为 0.35
- 如果使用中文拼音模型，需要先使用 `sherpa-onnx-cli text2token` 工具转换



### 4.3 完整示例（参考 MainActivity）

项目中的 `MainActivity.kt` 提供了一个完整的实现示例，包括：

- 权限申请（录音权限）
- 模型初始化
- 实时音频采集和处理
- UI 更新
- 资源管理

关键代码片段：

```kotlin
// 初始化模型
private fun initModel() {
    val type = 0 // 0: 中文模型, 1: 英文模型
    val config = KeywordSpotterConfig(
        featConfig = getFeatureConfig(
            sampleRate = sampleRateInHz,
            featureDim = 80
        ),
        modelConfig = getKwsModelConfig(type = type)!!,
        keywordsFile = getKeywordsFile(type = type),
    )
    
    kws = KeywordSpotter(
        assetManager = application.assets,
        config = config
    )
}

// 创建流并开始录音
stream = kws.createStream(keywords)
audioRecord!!.startRecording()

// 处理音频并检测关键词
while (isRecording) {
    val ret = audioRecord?.read(buffer, 0, buffer.size)
    if (ret != null && ret > 0) {
        val samples = FloatArray(ret) { buffer[it] / 32768.0f }
        stream.acceptWaveform(samples, sampleRate = sampleRateInHz)
        
        while (kws.isReady(stream)) {
            kws.decode(stream)
            val result = kws.getResult(stream)
            
            if (result.keyword.isNotBlank()) {
                // 处理检测到的关键词
                kws.reset(stream) // 重要：重置流
            }
        }
    }
}
```

### 4.4 注意事项

1. **采样率**：必须使用 16000 Hz 的采样率
2. **音频格式**：使用 16-bit PCM 格式
3. **重置流**：检测到关键词后必须调用 `kws.reset(stream)`，否则会影响后续检测
4. **资源释放**：使用完毕后记得释放 `OnlineStream` 和 `KeywordSpotter` 资源
5. **线程安全**：音频处理应在后台线程进行，UI 更新在主线程
6. **权限**：需要 `RECORD_AUDIO` 权限

### 4.5 性能优化建议

- 调整 `maxActivePaths`：值越大检测越准确，但计算量也越大
- 调整 `keywordsScore` 和 `keywordsThreshold`：根据实际场景平衡触发率和误报率
- 使用合适的模型：根据目标语言选择合适的模型
- 批量处理：可以累积一定数量的音频样本后再处理，减少调用次数

## 五、参考资料

- [官方文档](https://k2-fsa.github.io/sherpa/onnx/kws/index.html)
- [预训练模型列表](https://k2-fsa.github.io/sherpa/onnx/kws/pretrained_models/index.html)
- [GitHub 仓库](https://github.com/k2-fsa/sherpa-onnx)
- [Android 示例应用](https://k2-fsa.github.io/sherpa/onnx/kws/apk.html)

